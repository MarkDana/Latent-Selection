# Latent Variable Causal Discovery under Selection Bias

[Paper](https://openreview.net/pdf?id=W9YdVrSJIh) by [Haoyue Dai](https://hyda.cc), [Yiwen Qiu](https://evieq01.github.io/evieqiu.github.io/), [Ignavier Ng](https://ignavierng.github.io/), [Xinshuai Dong](https://dongxinshuai.github.io), [Peter Spirtes](https://www.cmu.edu/dietrich/philosophy/people/faculty/spirtes.html), [Kun Zhang](https://www.andrew.cmu.edu/user/kunz1/index.html). Appears at ICML 2025.


## The Assumptions of using Latent-Selection-Discovery Algorithm
 + Latent variables $L=(L_1,\cdots,L_k)$ are originally (before selection) generated by a linear Gaussian SEM: $L=BL+E_L$, where
   + $E_L$ are mutually independent noise components that **need to be Gaussian,** and
   + $B$ is the adjacency matrix among $L$ that corresponds to an arbitrary DAG.
 + Selection is applied to $L$ variables. Specifically,
   + There can be multiple selection processes acting simultaneously and independently.
   + Each $i$-th single selection is described by a configuration tuple $(L^{(i)}, \beta_i, \epsilon_i, \mathcal{Y}_i)$, where
     + $L^{(i)} \subseteq L$ is the subset of variables from $L$ directly involved in this $i$-th selection,
     + $\beta_i$ is a vector of nonzero linear coefficients that specifies how variables in $L^{(i)}$ contribute to the selection,
     + $\epsilon_i$ is an independent noise term that models selection randomness. **Note: $\epsilon_i$ needs not to be Gaussian;** and
     + $\mathcal{Y}_i \subsetneq \mathbb{R}$ is the set of admissible values, a proper subset of $\mathbb{R}$. **Note: it doesn't have to be a single-point selection:** $\mathcal{Y}_i$ can be a single value, multiple values, an interval, or a union of intervals, etc.
   + Finally, a data sample is included if and only if $Y_i \in \mathcal{Y}_i$ for all such $i$ s.
 + Observed variables $X=(X_1,\cdots,X_m)$ are latent variables' pure measurements, i.e. the "one-factor model", where
   + Each $X_j = c_j L_i + E_j$ for some $i$, and $E_j$ is an independent noise component that **needs not to be Gaussian;** and
   + Each $L_i$ has at least two such "pure measurements".
   + Note: though we give the general characterization of rank constraints without structural assumptions (Theorem 1), we need such "one-factor model" assumption in the current discovery algorithm.

## Running Latent-Selection-Discovery Algorithm on Your Own Data

Your input is expected to be:
 + `X_data`: the observed dataset, a `np.ndarray` in shape `(n_samples, n_measured_vars)`.
 + (Optional) `Lid_to_Xids`: the correspondence from latent variables to their pure measurements, which should form a partition of $X$.
   + Specify it if you know such correspondence in prior (e.g., from questionnaires). Otherwise, it will be estimated from `X_data`.

The output contains:
+ `estimated_Lid_to_Xids`: the estimated correspondence from latent variables to their pure measurements, if not specified in prior.
+ `estimated_L_PAG_edges`: the estimated partial ancestral graph (PAG) among $L$ variables subject to selection. The algorithm is like running FCI on $L$ as if we have access to them.

For reproduce such entire procedure, run `python main.py` for an example.

---

## Citation

If you use this code for your research, please cite our paper:

```bibtex
@inproceedings{
  dai2025latent,
  title={Latent Variable Causal Discovery under Selection Bias},
  author={Haoyue Dai and Yiwen Qiu and Ignavier Ng and Xinshuai Dong and Peter Spirtes and Kun Zhang},
  booktitle={Forty-second International Conference on Machine Learning},
  year={2025},
  url={https://openreview.net/forum?id=W9YdVrSJIh}
}
```

